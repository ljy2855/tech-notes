
### Human language, Word meaning
**usage (application)** 
- machine translation
- search 
- LLM
- image->text explain

**meaning**
- ì¶”ìƒì ì¸ ê°œë…ì„ ì–´ë–»ê²Œ í‘œí˜„í•´ì•¼í•˜ë‚˜
- symbolize ê°€ëŠ¥í• ê¹Œ?

**WordNet**
- ë™ì˜ì–´(synonym), ìƒìœ„ì–´(hypernyms) 
- ê°ê°ì˜ ë‹¨ì–´ê°€ ì–´ë–¤ ê´€ê³„ì¸ì§€ mapping
- í•œë²ˆ êµ¬ì¶•í•˜ë©´ ë°”ê¾¸ê¸°ê°€ ì–´ë ¤ì›€
- ì£¼ê´€ì ì¸ í•´ì„ì— ë”°ë¼ ë‹¤ë¦„

**Representing words as discrete symbols**

one-hot vector[^1]ë¡œ ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ í‘œí˜„í•œë‹¤
![[Pasted image 20250312165919.png]]
vector dimensionì€ vocabulary ê°œìˆ˜


ë¬¸ì œ ì—†ëƒ?
`Seattle motel` , `Seattle hotel`ì˜ ìœ ì‚¬ì„±ì„ ì°¾ê¸°ê°€ ì–´ë ¤ì›€
- ë‘ê°œì˜ ë²¡í„°ëŠ” **orthogonal**
- no natural notion of similarity for one-hot vectors

ê·¸ëŸ¼ ë‘ê°œì˜ ë²¡í„°ë¥¼ ì–´ë–»ê²Œ ì˜ë¯¸ìƒìœ¼ë¡œ ìœ ì‚¬í•˜ê²Œ í‘œí˜„í•  ìˆ˜ ìˆì„ê¹Œ

**Representing words by their context**

- Distributional semantics
	- ë™ì¼í•œ ë‹¨ì–´ì˜ ê·¼ì²˜ì— ìˆëŠ” context wordë¡œ ì˜ë¯¸ë¥¼ í‘œí˜„í•œë‹¤

### Word vectors
one-hot ì´ ì•„ë‹Œ `dense vector`ë¡œ í‘œí˜„
- ìœ ì‚¬í•œ ë‹¨ì–´ë“¤ì„ ê·¼ì²˜ì— ìœ„ì¹˜í•˜ê²Œ í•˜ê¸° ìœ„í•¨
![[Pasted image 20250312170615.png]]
![[Pasted image 20250312170859.png]]
- word embedding, word representationìœ¼ë¡œë„ ë¶ˆë¦¼

### Word2vec

![[Pasted image 20250312171211.png]]
- wordì˜ ì•ë’¤ ë‹¨ì–´ê°€ ë‚˜ì˜¬ í™•ë¥ ì„ ê³„ì‚°
- 


likelihood
![[Pasted image 20250312171847.png]]
objective function (loss function)
![[Pasted image 20250312171901.png]]


ğ‘ƒ(ğ‘¤ğ‘¡+ğ‘— | ğ‘¤ğ‘¡; ğœƒ) [^2]ëŠ” ì–´ë–»ê²Œ êµ¬í•˜ë‚˜ìš”?



prediction function
![[Pasted image 20250312172415.png]]

- softmax functionì„ í†µí•´ êµ¬í˜„
- o = íƒ€ê²Ÿ ë‹¨ì–´ (output word)
- c = ë¬¸ë§¥ ë‹¨ì–´ (context word)
- u_o = íƒ€ê²Ÿ ë‹¨ì–´ì˜ ë²¡í„°
- v_c = ë¬¸ë§¥ ë‹¨ì–´ì˜ ë²¡í„°
- u_w = ë‹¨ì–´ wì˜ ë²¡í„° (ì–´íœ˜ ì§‘í•© Vì— ì†í•œ ëª¨ë“  ë‹¨ì–´ì— ëŒ€í•´ ê³„ì‚°)

ì™œ softmaxë¥¼ ì“°ë‚˜ìš”??


Goal
- To train a model, we gradually adjust parameters to minimize a loss 
- Recall: ğœƒ represents all the model parameters, in one long vector
- 
![[Pasted image 20250312173201.png]]

Optimization: Gradient Descent
![[Pasted image 20250312172907.png]]
- ì´ˆê¸° ëœë¤ ê°’ ì‹œì‘
- ê¸°ìš¸ê¸°ê°€ 0ì´ ë˜ëŠ” ê³³ìœ¼ë¡œ ì´ë™í•˜ê²Œ í•¨
![[Pasted image 20250312173355.png]]
Stochastic Gradient Descent
- corpus ë‚´ ëª¨ë“  ë‹¨ì–´ì— ëŒ€í•´ì„œ lossê°€ ìµœì†Œë¡œ í•˜ë‹¤ë³´ë‹ˆ ë„ˆë¬´ ë§ìŒ
- mini batchë¥¼ ì¨ë³´ì!
	- í•œë²ˆì— ì „ì²´ê°€ ì•„ë‹ˆë¼, chunkë¥¼ ì§¤ë¼ì„œ í•˜ì
- ì—°ì‚°ëŸ‰ì„ ì¤„ì´ê¸°ë„ í•˜ë©´ì„œ, local optimizeì—ì„œ íƒˆì¶œ ê°€ëŠ¥


**gradientë¥¼ êµ¬í•´ë´…ì‹œë‹¹! ã…ã… ì‹œí—˜ ë‚˜ì˜¬ìˆ˜ë„ ìˆì–´ìš©**

[^1]: 0,1 ë¡œë§Œ ì´ë£¨ì–´ì§„ ë²¡í„°
[^2]: context(observated) word ë‚´ì—ì„œ center wordê°€ ë‚˜ì˜¬ í™•ë¥ 
