![[Lec10-transformers.pdf]]


attention
- sequence xë¥¼ í†µí•´ì„œ hidden layerë¡œ ì •ë³´ ì „ë‹¬í•¨
- ê·¸ëŸ¼ ì•„ì˜ˆ RNNì„ ì—†ì• ë³¼ê¹Œ?
- **Cross attention**: paying attention to the input x to generate ğ‘¦ğ‘¡


self attention
to generate ğ‘¦ğ‘¡, we need to pay attention to $ğ‘¦_{<ğ‘¡}$
- rnnê³¼ ë‹¤ë¥´ê²Œ $y_t$ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ì„œ ì´ì „ $ğ‘¦_{<ğ‘¡}$ ë¥¼ ì°¸ê³ í•œë‹¤!




position embedding


sinusoids

from scratch
- ì •í•´ì§„ index ë°–ì€ í‘œí˜„ì´ ì•ˆëŒ
- 


Decoder

![[Pasted image 20250409164756.png]]
- ë¯¸ë˜ ì •ë³´ëŠ” Maskingìœ¼ë¡œ 0ìœ¼ë¡œ ë§Œë“¬
- ì—¬ëŸ¬ Blockì„ ìŒ“ì•„ì„œ ë§Œë“¬
- Next tokenì˜ distr

Encoder
![[Pasted image 20250409164809.png]]
- bidirectionalí•˜ê¸° ìœ„í•´ No masking ì§„í–‰


![[Pasted image 20250409164934.png]]

- croess attention