![[Lec11-transformers2.pdf]]


### Positional Embeddings

ì™œ í•„ìš”í• ê¹Œ?

The **dog** chased another **dog**
-> ìœ„ì¹˜ ì •ë³´ê°€ ì—†ë‹¤ë©´, dog1ì™€ dog2ë¥¼ êµ¬ë³„ ë¶ˆê°€ëŠ¥

#### code
```python
import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel

model_id = "meta-llama/Llama-3.2-1B"
tok = AutoTokenizer.from_pretrained(model_id)
model = AutoModel.from_pretrained(model_id)

text = "The dog chased another dog"
tokens = tok(text, return_tensors="pt")["input_ids"]
embeddings = model.embed_tokens(tokens) # ê° í† í°ì— ëŒ€í•œ ì„ë² ë”© batch ì²˜ë¦¬
hdim = embeddings.shape[-1] # metrics size í™•ì¸


# y = wx
# forward, backward ì‚¬ìš© ê°€ëŠ¥
W_q = nn.Linear(hdim, hdim, bias=False) # query
W_k = nn.Linear(hdim, hdim, bias=False) # key
W_v = nn.Linear(hdim, hdim, bias=False) # value
mha = nn.MultiheadAttention(embed_dim=hdim, num_heads=4, batch_first=True) 

# mha layer ëª¨ë“  íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”
with torch.no_grad():
    for param in mha.parameters():
        nn.init.normal_(param, std=0.1) # Initialize weights to be non-negligible

output, _ = mha(W_q(embeddings), W_k(embeddings), W_v(embeddings))

dog1_out = output[0, 2]
dog2_out = output[0, 5]
print(f"Dog output identical?: {torch.allclose(dog1_out, dog2_out, atol=1e-6)}") #True


```

- tokenizer : vocabë¥¼ ë§Œë“œëŠ” ëª¨ë¸
	- word ë‹¨ìœ„ë¡œ ì§œë¥´ëŠ” ê²ƒë³´ë‹¤ tokenìœ¼ë¡œ ìë¥´ëŠ”ê²Œ ì¢‹ìŒ
- multi head attentionì„ í†µê³¼í•œ dogë¥¼ ê°™ë‚˜ ë‹¤ë¥¸ê°€ íŒë³„
- q,v,k
![[Pasted image 20250409170401.png]]


ì„ë² ë”©ì— ë¬´ìŠ¨ íŠ¹ì§•ì´ í•„ìš”í• ê¹Œ
- ê°ê°ì˜ unique í¬ì§€ì…˜ë“¤ì„ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆì–´ì•¼í•¨
- ìœ„ì¹˜ì˜ ì°¨ì´ì— ëŒ€í•œ ì •ë³´ ë°˜ì˜
- ê¸´ sequenceì— ëŒ€í•´ ì¼ë°˜í™” ê°€ëŠ¥
- ëœë¤ë˜ì§€ ì•Šì€ ê³¼ì •ì„ í†µí•´ ëª¨ë¸ì´ í•™ìŠµí•´ì•¼í•¨
- ë‹¤ì–‘í•œ ì°¨ì›ìœ¼ë¡œ í™•ì¥ ê°€ëŠ¥í•´ì•¼í•¨


**Integer Position Encoding**


**Binary Position Encoding**
- 2ì§„ìˆ˜ì²˜ëŸ¼  1,0 ë§Œìœ¼ë¡œ í‘œí˜„ -> range ì••ì¶•
- sinusoids ì™€ ë¹„ìŠ·í•œ ê°œë…
- position embeddingì´ ë„ˆë¬´ ë‚ ëœ€

![[Pasted image 20250409171117.png]]

**Sinusoidal Positional Encoding**
![[Pasted image 20250409171404.png]]
- relative position embeddingì„ ë§Œì¡±í•˜ì§„ ëª»í•¨


**RoPE**
![[Pasted image 20250409171521.png]]

- f,yê°€ ìƒëŒ€ì ì¸ positionìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆì—ˆìŒ ì¢‹ê²Ÿë„¤~


![[Pasted image 20250409171726.png]]
- inner productë¡œ absolute positionì´ ì•„ë‹Œ relative postionìœ¼ë¡œ í‘œí˜„í•˜ê¸¸ ì›í•¨


![[Pasted image 20250409172103.png]]

- position mì„ í†µí•´ ê¸°ì¡´ qeury/key vectorë“¤ì„ rotation ì‹œì¼œë²„ë¦¼
- í˜„ì¬ ë‹¨ê³„ì—ì„œ ê°€ì¥ ë§ì´ ì‚¬ìš©

ê·¸ë˜ì„œ ì§„ì§œ ì˜í•˜ë‚˜?

machine translation
`BLEU` : ë²ˆì—­ëœ referenceë‘ ì–¼ë§ˆë‚˜ ë¹„ìŠ·í•œê°€?
`FLOPs` : ì—°ì‚°ëŸ‰ì´ ì–¼ë§ˆë‚˜ ë§ë‚˜?

![[Pasted image 20250409172956.png]]

document generation

![[Pasted image 20250409173006.png]]

GLUE
downstream taskì—ì„œ ì–¼ë§ˆë‚˜ ì˜í•˜ëŠ” ì§€ ì¢…í•©ì ìœ¼ë¡œ íŒë‹¨

#### ê·¸ëŸ¼ ì´ê²Œ ì§±ì§±ì´ëƒ?

í•™ìŠµì„ ì–´ë–»ê²Œ ì•ˆì •í™”í• ê±´ê°€

- ëŒ€ë¶€ë¶„ ì—”ì§€ë‹ˆë§ ë¬¸ì œê°€ ìˆìŒ

pre vs post norm
![[Pasted image 20250409173314.png]]
- multi attention ì „ì— normalizationì„ ë¨¼ì € í•˜ëŠ”ê²Œ ì‹¤í—˜ì ìœ¼ë¡œ ë°œê²¬ë¨

**Quadratic computation**

![[Pasted image 20250409173428.png]]

- $ğ‘‚(n^2d)$ë§Œí¼ì˜ ì—°ì‚°ëŸ‰ì´ í•„ìš”í•¨
- ì¼ë°˜ì ì¸ LMì€ dëŠ” 1000ì„ ë„˜ì–´ê°
- sequenceê°€ ê¸¸ì–´ì§€ë‹¤ ë³´ë©´, ë¯¸ì¹œë“¯ì´ ì—°ì‚°ëŸ‰ì´ ëŠ˜ì–´ë‚¨


RNNìœ¼ë¡œ ëŒì•„ê°€ë³¼ê¹Œ..?

![[Pasted image 20250409173708.png]]

ëª¨ë¸ ì•„í‚¤í…ì²˜ë¥¼ ë°”ê¿”ë²„ë¦´ê¹Œ?

ì‹œìŠ¤í…œ ìµœì í™”ë¥¼ í•´ë³¼ê¹Œ? -> (Flash attention)

í˜„ì¬ ë‹¨ê³„ì—ì„œëŠ” ì„œë¡œ ì‹¸ìš°ëŠ”ì¤‘


### Application

![[Pasted image 20250409174103.png]]

Vision
![[Pasted image 20250409174041.png]]

Wav
![[Pasted image 20250409174117.png]]

ìŒì„± ë°ì´í„°ê°€ ë“¤ì–´ê°”ì„ ë•Œ,